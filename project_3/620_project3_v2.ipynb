{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and data\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "\n",
    "random.seed(1989)\n",
    "\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "random.shuffle(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features for the first classifier\n",
    "def gender_features_iter_1(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "7444\n"
     ]
    }
   ],
   "source": [
    "# Create test-train split\n",
    "test_names = names[:500]\n",
    "trainset_names = names[500:]\n",
    "#AT THE END!\n",
    "#test_featuresets = [(gender_features_iter_?(n), g) for (n,g) in test_names]\n",
    "\n",
    "print(len(test_names))\n",
    "print(len(trainset_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.37427        0.761\n",
      "             3          -0.37386        0.761\n",
      "             4          -0.37360        0.761\n",
      "             5          -0.37344        0.761\n",
      "             6          -0.37332        0.761\n",
      "             7          -0.37323        0.761\n",
      "             8          -0.37316        0.761\n",
      "             9          -0.37310        0.761\n",
      "            10          -0.37306        0.761\n",
      "            11          -0.37302        0.761\n",
      "            12          -0.37299        0.761\n",
      "            13          -0.37296        0.761\n",
      "            14          -0.37294        0.761\n",
      "            15          -0.37291        0.761\n",
      "            16          -0.37290        0.761\n",
      "            17          -0.37288        0.761\n",
      "            18          -0.37286        0.761\n",
      "            19          -0.37285        0.761\n",
      "            20          -0.37284        0.761\n",
      "            21          -0.37283        0.761\n",
      "            22          -0.37282        0.761\n",
      "            23          -0.37281        0.761\n",
      "            24          -0.37280        0.761\n",
      "            25          -0.37279        0.761\n",
      "            26          -0.37279        0.761\n",
      "            27          -0.37278        0.761\n",
      "            28          -0.37277        0.761\n",
      "            29          -0.37277        0.761\n",
      "            30          -0.37276        0.761\n",
      "            31          -0.37276        0.761\n",
      "            32          -0.37275        0.761\n",
      "            33          -0.37275        0.761\n",
      "            34          -0.37274        0.761\n",
      "            35          -0.37274        0.761\n",
      "            36          -0.37274        0.761\n",
      "            37          -0.37273        0.761\n",
      "            38          -0.37273        0.761\n",
      "            39          -0.37273        0.761\n",
      "            40          -0.37272        0.761\n",
      "            41          -0.37272        0.761\n",
      "            42          -0.37272        0.761\n",
      "            43          -0.37271        0.761\n",
      "            44          -0.37271        0.761\n",
      "            45          -0.37271        0.761\n",
      "            46          -0.37271        0.761\n",
      "            47          -0.37271        0.761\n",
      "            48          -0.37270        0.761\n",
      "            49          -0.37270        0.761\n",
      "            50          -0.37270        0.761\n",
      "            51          -0.37270        0.761\n",
      "            52          -0.37270        0.761\n",
      "            53          -0.37269        0.761\n",
      "            54          -0.37269        0.761\n",
      "            55          -0.37269        0.761\n",
      "            56          -0.37269        0.761\n",
      "            57          -0.37269        0.761\n",
      "            58          -0.37269        0.761\n",
      "            59          -0.37268        0.761\n",
      "            60          -0.37268        0.761\n",
      "            61          -0.37268        0.761\n",
      "            62          -0.37268        0.761\n",
      "            63          -0.37268        0.761\n",
      "            64          -0.37268        0.761\n",
      "            65          -0.37268        0.761\n",
      "            66          -0.37268        0.761\n",
      "            67          -0.37267        0.761\n",
      "            68          -0.37267        0.761\n",
      "            69          -0.37267        0.761\n",
      "            70          -0.37267        0.761\n",
      "            71          -0.37267        0.761\n",
      "            72          -0.37267        0.761\n",
      "            73          -0.37267        0.761\n",
      "            74          -0.37267        0.761\n",
      "            75          -0.37267        0.761\n",
      "            76          -0.37267        0.761\n",
      "            77          -0.37266        0.761\n",
      "            78          -0.37266        0.761\n",
      "            79          -0.37266        0.761\n",
      "            80          -0.37266        0.761\n",
      "            81          -0.37266        0.761\n",
      "            82          -0.37266        0.761\n",
      "            83          -0.37266        0.761\n",
      "            84          -0.37266        0.761\n",
      "            85          -0.37266        0.761\n",
      "            86          -0.37266        0.761\n",
      "            87          -0.37266        0.761\n",
      "            88          -0.37266        0.761\n",
      "            89          -0.37266        0.761\n",
      "            90          -0.37266        0.761\n",
      "            91          -0.37266        0.761\n",
      "            92          -0.37265        0.761\n",
      "            93          -0.37265        0.761\n",
      "            94          -0.37265        0.761\n",
      "            95          -0.37265        0.761\n",
      "            96          -0.37265        0.761\n",
      "            97          -0.37265        0.761\n",
      "            98          -0.37265        0.761\n",
      "            99          -0.37265        0.761\n",
      "         Final          -0.37265        0.761\n",
      "0.752\n"
     ]
    }
   ],
   "source": [
    "# Create first devtest and training set split\n",
    "random.shuffle(trainset_names)\n",
    "devtest_names_1 = trainset_names[:500]\n",
    "train_names_1 = trainset_names[500:]\n",
    "devtest_featuresets_1 = [(gender_features_iter_1(n), g) for (n,g) in devtest_names_1]\n",
    "train_featuresets_1 = [(gender_features_iter_1(n), g) for (n,g) in train_names_1]\n",
    "\n",
    "# Train the classifier on the training set and print the accuracy\n",
    "classifier_1 = nltk.MaxentClassifier.train(train_featuresets_1)\n",
    "print(nltk.classify.accuracy(classifier_1, devtest_featuresets_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.43199        0.760\n",
      "             3          -0.38678        0.780\n",
      "             4          -0.36920        0.782\n",
      "             5          -0.36092        0.780\n",
      "             6          -0.35656        0.783\n",
      "             7          -0.35410        0.783\n",
      "             8          -0.35262        0.784\n",
      "             9          -0.35169        0.784\n",
      "            10          -0.35108        0.784\n",
      "            11          -0.35067        0.784\n",
      "            12          -0.35037        0.784\n",
      "            13          -0.35016        0.784\n",
      "            14          -0.35000        0.784\n",
      "            15          -0.34987        0.784\n",
      "            16          -0.34978        0.784\n",
      "            17          -0.34970        0.784\n",
      "            18          -0.34964        0.784\n",
      "            19          -0.34959        0.784\n",
      "            20          -0.34955        0.784\n",
      "            21          -0.34952        0.784\n",
      "            22          -0.34949        0.784\n",
      "            23          -0.34946        0.784\n",
      "            24          -0.34944        0.784\n",
      "            25          -0.34942        0.784\n",
      "            26          -0.34940        0.784\n",
      "            27          -0.34938        0.784\n",
      "            28          -0.34937        0.784\n",
      "            29          -0.34936        0.784\n",
      "            30          -0.34934        0.784\n",
      "            31          -0.34933        0.784\n",
      "            32          -0.34932        0.784\n",
      "            33          -0.34931        0.784\n",
      "            34          -0.34931        0.784\n",
      "            35          -0.34930        0.784\n",
      "            36          -0.34929        0.784\n",
      "            37          -0.34928        0.784\n",
      "            38          -0.34928        0.784\n",
      "            39          -0.34927        0.784\n",
      "            40          -0.34926        0.784\n",
      "            41          -0.34926        0.784\n",
      "            42          -0.34925        0.784\n",
      "            43          -0.34925        0.784\n",
      "            44          -0.34924        0.784\n",
      "            45          -0.34924        0.784\n",
      "            46          -0.34923        0.784\n",
      "            47          -0.34923        0.784\n",
      "            48          -0.34922        0.784\n",
      "            49          -0.34922        0.784\n",
      "            50          -0.34922        0.784\n",
      "            51          -0.34921        0.784\n",
      "            52          -0.34921        0.784\n",
      "            53          -0.34921        0.784\n",
      "            54          -0.34920        0.784\n",
      "            55          -0.34920        0.784\n",
      "            56          -0.34920        0.784\n",
      "            57          -0.34919        0.784\n",
      "            58          -0.34919        0.784\n",
      "            59          -0.34919        0.784\n",
      "            60          -0.34919        0.784\n",
      "            61          -0.34918        0.784\n",
      "            62          -0.34918        0.784\n",
      "            63          -0.34918        0.784\n",
      "            64          -0.34918        0.784\n",
      "            65          -0.34917        0.784\n",
      "            66          -0.34917        0.784\n",
      "            67          -0.34917        0.784\n",
      "            68          -0.34917        0.784\n",
      "            69          -0.34917        0.784\n",
      "            70          -0.34916        0.784\n",
      "            71          -0.34916        0.784\n",
      "            72          -0.34916        0.784\n",
      "            73          -0.34916        0.784\n",
      "            74          -0.34916        0.784\n",
      "            75          -0.34915        0.784\n",
      "            76          -0.34915        0.784\n",
      "            77          -0.34915        0.784\n",
      "            78          -0.34915        0.784\n",
      "            79          -0.34915        0.784\n",
      "            80          -0.34915        0.784\n",
      "            81          -0.34915        0.784\n",
      "            82          -0.34914        0.784\n",
      "            83          -0.34914        0.784\n",
      "            84          -0.34914        0.784\n",
      "            85          -0.34914        0.784\n",
      "            86          -0.34914        0.784\n",
      "            87          -0.34914        0.784\n",
      "            88          -0.34914        0.784\n",
      "            89          -0.34913        0.784\n",
      "            90          -0.34913        0.784\n",
      "            91          -0.34913        0.784\n",
      "            92          -0.34913        0.784\n",
      "            93          -0.34913        0.784\n",
      "            94          -0.34913        0.784\n",
      "            95          -0.34913        0.784\n",
      "            96          -0.34913        0.784\n",
      "            97          -0.34913        0.784\n",
      "            98          -0.34913        0.784\n",
      "            99          -0.34912        0.784\n",
      "         Final          -0.34912        0.784\n",
      "0.752\n"
     ]
    }
   ],
   "source": [
    "# Define the features for the second classifier\n",
    "def gender_features_iter_2(word):\n",
    "    return {'first_letter': word[0].lower(),\n",
    "            'last_letter': word[-1]}\n",
    "\n",
    "# Create second devtest and training set split\n",
    "random.shuffle(trainset_names)\n",
    "devtest_names_2 = trainset_names[:500]\n",
    "train_names_2 = trainset_names[500:]\n",
    "devtest_featuresets_2 = [(gender_features_iter_2(n), g) for (n,g) in devtest_names_2]\n",
    "train_featuresets_2 = [(gender_features_iter_2(n), g) for (n,g) in train_names_2]\n",
    "\n",
    "# Train the classifier on the training set and print the accuracy\n",
    "classifier_2 = nltk.MaxentClassifier.train(train_featuresets_2)\n",
    "print(nltk.classify.accuracy(classifier_2, devtest_featuresets_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.369\n",
      "             2          -0.44756        0.751\n",
      "             3          -0.40374        0.769\n",
      "             4          -0.38703        0.765\n",
      "             5          -0.37956        0.765\n",
      "             6          -0.37590        0.765\n",
      "             7          -0.37399        0.765\n",
      "             8          -0.37294        0.765\n",
      "             9          -0.37233        0.765\n",
      "            10          -0.37196        0.765\n",
      "            11          -0.37171        0.768\n",
      "            12          -0.37155        0.768\n",
      "            13          -0.37143        0.768\n",
      "            14          -0.37133        0.768\n",
      "            15          -0.37126        0.768\n",
      "            16          -0.37121        0.768\n",
      "            17          -0.37116        0.768\n",
      "            18          -0.37112        0.768\n",
      "            19          -0.37108        0.768\n",
      "            20          -0.37105        0.768\n",
      "            21          -0.37103        0.768\n",
      "            22          -0.37101        0.768\n",
      "            23          -0.37099        0.768\n",
      "            24          -0.37097        0.768\n",
      "            25          -0.37095        0.768\n",
      "            26          -0.37093        0.768\n",
      "            27          -0.37092        0.768\n",
      "            28          -0.37091        0.768\n",
      "            29          -0.37089        0.768\n",
      "            30          -0.37088        0.768\n",
      "            31          -0.37087        0.768\n",
      "            32          -0.37086        0.768\n",
      "            33          -0.37085        0.768\n",
      "            34          -0.37084        0.768\n",
      "            35          -0.37084        0.768\n",
      "            36          -0.37083        0.768\n",
      "            37          -0.37082        0.768\n",
      "            38          -0.37081        0.768\n",
      "            39          -0.37081        0.768\n",
      "            40          -0.37080        0.768\n",
      "            41          -0.37080        0.768\n",
      "            42          -0.37079        0.768\n",
      "            43          -0.37078        0.768\n",
      "            44          -0.37078        0.768\n",
      "            45          -0.37077        0.768\n",
      "            46          -0.37077        0.768\n",
      "            47          -0.37076        0.768\n",
      "            48          -0.37076        0.768\n",
      "            49          -0.37076        0.768\n",
      "            50          -0.37075        0.768\n",
      "            51          -0.37075        0.768\n",
      "            52          -0.37074        0.768\n",
      "            53          -0.37074        0.768\n",
      "            54          -0.37074        0.768\n",
      "            55          -0.37073        0.768\n",
      "            56          -0.37073        0.768\n",
      "            57          -0.37073        0.768\n",
      "            58          -0.37072        0.768\n",
      "            59          -0.37072        0.768\n",
      "            60          -0.37072        0.768\n",
      "            61          -0.37072        0.768\n",
      "            62          -0.37071        0.768\n",
      "            63          -0.37071        0.768\n",
      "            64          -0.37071        0.768\n",
      "            65          -0.37071        0.768\n",
      "            66          -0.37070        0.768\n",
      "            67          -0.37070        0.768\n",
      "            68          -0.37070        0.768\n",
      "            69          -0.37070        0.768\n",
      "            70          -0.37069        0.768\n",
      "            71          -0.37069        0.768\n",
      "            72          -0.37069        0.768\n",
      "            73          -0.37069        0.768\n",
      "            74          -0.37069        0.768\n",
      "            75          -0.37068        0.768\n",
      "            76          -0.37068        0.768\n",
      "            77          -0.37068        0.768\n",
      "            78          -0.37068        0.768\n",
      "            79          -0.37068        0.768\n",
      "            80          -0.37068        0.768\n",
      "            81          -0.37067        0.768\n",
      "            82          -0.37067        0.768\n",
      "            83          -0.37067        0.768\n",
      "            84          -0.37067        0.768\n",
      "            85          -0.37067        0.768\n",
      "            86          -0.37067        0.768\n",
      "            87          -0.37067        0.768\n",
      "            88          -0.37066        0.768\n",
      "            89          -0.37066        0.768\n",
      "            90          -0.37066        0.768\n",
      "            91          -0.37066        0.768\n",
      "            92          -0.37066        0.768\n",
      "            93          -0.37066        0.768\n",
      "            94          -0.37066        0.768\n",
      "            95          -0.37066        0.768\n",
      "            96          -0.37066        0.768\n",
      "            97          -0.37065        0.768\n",
      "            98          -0.37065        0.768\n",
      "            99          -0.37065        0.768\n",
      "         Final          -0.37065        0.768\n",
      "0.756\n"
     ]
    }
   ],
   "source": [
    "# Define the features for the third classifier\n",
    "def gender_features_iter_3(word):\n",
    "    return {'last_letter': word[-1],\n",
    "            'length_of_name': len(word)}\n",
    "\n",
    "# Create third devtest and training set split\n",
    "random.shuffle(trainset_names)\n",
    "devtest_names_3 = trainset_names[:500]\n",
    "train_names_3 = trainset_names[500:]\n",
    "devtest_featuresets_3 = [(gender_features_iter_3(n), g) for (n,g) in devtest_names_3]\n",
    "train_featuresets_3 = [(gender_features_iter_3(n), g) for (n,g) in train_names_3]\n",
    "\n",
    "# Train the classifier on the training set and print the accuracy\n",
    "classifier_3 = nltk.MaxentClassifier.train(train_featuresets_3)\n",
    "print(nltk.classify.accuracy(classifier_3, devtest_featuresets_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.48096        0.731\n",
      "             3          -0.42465        0.772\n",
      "             4          -0.39682        0.774\n",
      "             5          -0.38141        0.778\n",
      "             6          -0.37210        0.778\n",
      "             7          -0.36613        0.780\n",
      "             8          -0.36213        0.780\n",
      "             9          -0.35936        0.781\n",
      "            10          -0.35740        0.781\n",
      "            11          -0.35598        0.781\n",
      "            12          -0.35492        0.782\n",
      "            13          -0.35413        0.782\n",
      "            14          -0.35353        0.782\n",
      "            15          -0.35306        0.781\n",
      "            16          -0.35269        0.781\n",
      "            17          -0.35239        0.781\n",
      "            18          -0.35215        0.781\n",
      "            19          -0.35196        0.780\n",
      "            20          -0.35180        0.780\n",
      "            21          -0.35167        0.780\n",
      "            22          -0.35156        0.780\n",
      "            23          -0.35147        0.780\n",
      "            24          -0.35139        0.780\n",
      "            25          -0.35132        0.780\n",
      "            26          -0.35126        0.780\n",
      "            27          -0.35121        0.780\n",
      "            28          -0.35116        0.780\n",
      "            29          -0.35113        0.780\n",
      "            30          -0.35109        0.780\n",
      "            31          -0.35106        0.780\n",
      "            32          -0.35103        0.780\n",
      "            33          -0.35101        0.780\n",
      "            34          -0.35099        0.780\n",
      "            35          -0.35097        0.780\n",
      "            36          -0.35095        0.780\n",
      "            37          -0.35093        0.780\n",
      "            38          -0.35092        0.780\n",
      "            39          -0.35090        0.780\n",
      "            40          -0.35089        0.780\n",
      "            41          -0.35088        0.780\n",
      "            42          -0.35087        0.780\n",
      "            43          -0.35085        0.780\n",
      "            44          -0.35084        0.780\n",
      "            45          -0.35084        0.780\n",
      "            46          -0.35083        0.780\n",
      "            47          -0.35082        0.780\n",
      "            48          -0.35081        0.780\n",
      "            49          -0.35080        0.780\n",
      "            50          -0.35080        0.780\n",
      "            51          -0.35079        0.780\n",
      "            52          -0.35078        0.780\n",
      "            53          -0.35078        0.780\n",
      "            54          -0.35077        0.780\n",
      "            55          -0.35076        0.780\n",
      "            56          -0.35076        0.780\n",
      "            57          -0.35075        0.780\n",
      "            58          -0.35075        0.780\n",
      "            59          -0.35074        0.780\n",
      "            60          -0.35074        0.780\n",
      "            61          -0.35073        0.780\n",
      "            62          -0.35073        0.780\n",
      "            63          -0.35073        0.780\n",
      "            64          -0.35072        0.780\n",
      "            65          -0.35072        0.780\n",
      "            66          -0.35071        0.780\n",
      "            67          -0.35071        0.780\n",
      "            68          -0.35071        0.780\n",
      "            69          -0.35070        0.780\n",
      "            70          -0.35070        0.780\n",
      "            71          -0.35070        0.780\n",
      "            72          -0.35069        0.780\n",
      "            73          -0.35069        0.780\n",
      "            74          -0.35069        0.780\n",
      "            75          -0.35068        0.780\n",
      "            76          -0.35068        0.780\n",
      "            77          -0.35068        0.780\n",
      "            78          -0.35068        0.780\n",
      "            79          -0.35067        0.780\n",
      "            80          -0.35067        0.780\n",
      "            81          -0.35067        0.780\n",
      "            82          -0.35067        0.780\n",
      "            83          -0.35066        0.780\n",
      "            84          -0.35066        0.780\n",
      "            85          -0.35066        0.780\n",
      "            86          -0.35066        0.780\n",
      "            87          -0.35066        0.780\n",
      "            88          -0.35065        0.780\n",
      "            89          -0.35065        0.780\n",
      "            90          -0.35065        0.780\n",
      "            91          -0.35065        0.780\n",
      "            92          -0.35065        0.780\n",
      "            93          -0.35064        0.780\n",
      "            94          -0.35064        0.780\n",
      "            95          -0.35064        0.780\n",
      "            96          -0.35064        0.780\n",
      "            97          -0.35064        0.780\n",
      "            98          -0.35063        0.780\n",
      "            99          -0.35063        0.780\n",
      "         Final          -0.35063        0.780\n",
      "0.792\n"
     ]
    }
   ],
   "source": [
    "# Define the features for the fourth classifier\n",
    "def gender_features_iter_4(word):\n",
    "    return {'first_letter': word[0].lower(),\n",
    "            'last_letter': word[-1],\n",
    "            'length_of_name': len(word)}\n",
    "\n",
    "# Create fourth devtest and training set split\n",
    "random.shuffle(trainset_names)\n",
    "devtest_names_4 = trainset_names[:500]\n",
    "train_names_4 = trainset_names[500:]\n",
    "devtest_featuresets_4 = [(gender_features_iter_4(n), g) for (n,g) in devtest_names_4]\n",
    "train_featuresets_4 = [(gender_features_iter_4(n), g) for (n,g) in train_names_4]\n",
    "\n",
    "# Train the classifier on the training set and print the accuracy\n",
    "classifier_4 = nltk.MaxentClassifier.train(train_featuresets_4)\n",
    "print(nltk.classify.accuracy(classifier_4, devtest_featuresets_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Errors: ['Rutledge', 'Vinny', 'Vite', 'Rodolphe', 'Johnnie', 'Spense', 'Tremayne', 'Daryl', 'Jerrome', 'Emery', 'Stearne', 'Ave', 'Uli', 'Carlie', 'Meryl', 'Nicky', 'Terence', 'Jephthah', 'Merrill', 'Michale', 'Rodolph', 'Roderich', 'Conroy', 'Nichole', 'Frederich', 'Lance', 'Laurance', 'Clemente', 'Clyde', 'Connie', 'Dwaine', 'Louie', 'Stanleigh', 'Barry', 'Eddie', 'Carmine', 'Georgia', 'Joshua', 'Immanuel', 'Vale', 'Carsten', 'Aristotle', 'Rourke', 'Calvin', 'Adlai', 'Mikhail', 'Chelton', 'Nathaniel', 'Bealle', 'Olle', 'Bertie', 'Jamie', 'Cobbie', 'Mendel', 'Durante', 'Logan', 'Augie', 'Fletch', 'Saxe', 'Chrisy', 'Kelly']\n",
      "Female Errors:  ['Sinead', 'Pauly', 'Danit', 'Mab', 'Frances', 'Haley', 'Marlo', 'Lind', 'Cass', 'Hatty', 'Robby', 'Phillis', 'Shawn', 'Holly', 'Dyann', 'Marylou', 'Sybyl', 'Aimil', 'Tracey', 'Stormy', 'Hester', 'Rhianon', 'Elyn', 'Dion', 'Clo', 'Sigrid', 'Alisun', 'Ginnifer', 'Hilliary', 'Sara-Ann', 'Margret', 'Fawn', 'Ruby', 'Angel', 'Scarlet', 'Sally', 'Joleen', 'Marget', 'Tess', 'Nert', 'Bidget', 'Shell', 'Tracy']\n"
     ]
    }
   ],
   "source": [
    "# Create list of devtest errors from iteration 4\n",
    "errors_name_4 = []\n",
    "errors_tag_4 = []\n",
    "errors_guess_4 = []\n",
    "\n",
    "for (name, tag) in devtest_names_4:\n",
    "    guess = classifier_4.classify(gender_features_iter_4(name))\n",
    "    if guess != tag:\n",
    "        errors_name_4.append(name)\n",
    "        errors_tag_4.append(tag)\n",
    "        errors_guess_4.append(guess)\n",
    "\n",
    "# Create DataFrame of errors\n",
    "import pandas as pd\n",
    "errors_4_df = pd.DataFrame({'name': errors_name_4,\n",
    "                          'tag': errors_tag_4,\n",
    "                          'guess': errors_guess_4})\n",
    "\n",
    "# Print male errors\n",
    "print('Male Errors:',\n",
    "      errors_4_df[errors_4_df.tag == 'male']['name'].tolist())\n",
    "\n",
    "# Print female errors\n",
    "print('Female Errors: ',\n",
    "      errors_4_df[errors_4_df.tag == 'female']['name'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that checks if a word contains a double letter\n",
    "def double_letter_check(word):\n",
    "    for i in range(len(word) - 1):\n",
    "        if word[i] == word[i + 1]:\n",
    "            return(True)\n",
    "    return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.50703        0.699\n",
      "             3          -0.44844        0.768\n",
      "             4          -0.41483        0.780\n",
      "             5          -0.39438        0.782\n",
      "             6          -0.38115        0.786\n",
      "             7          -0.37216        0.787\n",
      "             8          -0.36581        0.787\n",
      "             9          -0.36119        0.787\n",
      "            10          -0.35774        0.788\n",
      "            11          -0.35513        0.788\n",
      "            12          -0.35311        0.787\n",
      "            13          -0.35153        0.787\n",
      "            14          -0.35029        0.787\n",
      "            15          -0.34929        0.787\n",
      "            16          -0.34848        0.788\n",
      "            17          -0.34783        0.788\n",
      "            18          -0.34729        0.788\n",
      "            19          -0.34684        0.788\n",
      "            20          -0.34647        0.788\n",
      "            21          -0.34616        0.788\n",
      "            22          -0.34590        0.787\n",
      "            23          -0.34567        0.787\n",
      "            24          -0.34548        0.787\n",
      "            25          -0.34532        0.787\n",
      "            26          -0.34518        0.787\n",
      "            27          -0.34506        0.787\n",
      "            28          -0.34495        0.787\n",
      "            29          -0.34486        0.787\n",
      "            30          -0.34478        0.787\n",
      "            31          -0.34471        0.787\n",
      "            32          -0.34465        0.787\n",
      "            33          -0.34459        0.787\n",
      "            34          -0.34455        0.787\n",
      "            35          -0.34450        0.787\n",
      "            36          -0.34446        0.787\n",
      "            37          -0.34443        0.787\n",
      "            38          -0.34440        0.787\n",
      "            39          -0.34437        0.787\n",
      "            40          -0.34434        0.787\n",
      "            41          -0.34432        0.787\n",
      "            42          -0.34430        0.787\n",
      "            43          -0.34428        0.787\n",
      "            44          -0.34426        0.787\n",
      "            45          -0.34425        0.787\n",
      "            46          -0.34423        0.787\n",
      "            47          -0.34422        0.787\n",
      "            48          -0.34421        0.787\n",
      "            49          -0.34420        0.787\n",
      "            50          -0.34418        0.787\n",
      "            51          -0.34417        0.787\n",
      "            52          -0.34417        0.787\n",
      "            53          -0.34416        0.787\n",
      "            54          -0.34415        0.787\n",
      "            55          -0.34414        0.787\n",
      "            56          -0.34413        0.787\n",
      "            57          -0.34413        0.787\n",
      "            58          -0.34412        0.787\n",
      "            59          -0.34411        0.787\n",
      "            60          -0.34411        0.787\n",
      "            61          -0.34410        0.787\n",
      "            62          -0.34410        0.787\n",
      "            63          -0.34409        0.787\n",
      "            64          -0.34409        0.787\n",
      "            65          -0.34408        0.787\n",
      "            66          -0.34408        0.787\n",
      "            67          -0.34407        0.787\n",
      "            68          -0.34407        0.787\n",
      "            69          -0.34407        0.787\n",
      "            70          -0.34406        0.787\n",
      "            71          -0.34406        0.787\n",
      "            72          -0.34406        0.787\n",
      "            73          -0.34405        0.787\n",
      "            74          -0.34405        0.787\n",
      "            75          -0.34405        0.787\n",
      "            76          -0.34404        0.787\n",
      "            77          -0.34404        0.787\n",
      "            78          -0.34404        0.787\n",
      "            79          -0.34403        0.787\n",
      "            80          -0.34403        0.787\n",
      "            81          -0.34403        0.787\n",
      "            82          -0.34403        0.787\n",
      "            83          -0.34402        0.787\n",
      "            84          -0.34402        0.787\n",
      "            85          -0.34402        0.787\n",
      "            86          -0.34402        0.787\n",
      "            87          -0.34401        0.787\n",
      "            88          -0.34401        0.787\n",
      "            89          -0.34401        0.787\n",
      "            90          -0.34401        0.787\n",
      "            91          -0.34401        0.787\n",
      "            92          -0.34400        0.787\n",
      "            93          -0.34400        0.787\n",
      "            94          -0.34400        0.787\n",
      "            95          -0.34400        0.787\n",
      "            96          -0.34400        0.787\n",
      "            97          -0.34399        0.787\n",
      "            98          -0.34399        0.787\n",
      "            99          -0.34399        0.787\n",
      "         Final          -0.34399        0.787\n",
      "0.776\n"
     ]
    }
   ],
   "source": [
    "# Define the features for the fifth classifier\n",
    "def gender_features_iter_5(word):\n",
    "    return {'first_letter': word[0].lower(),\n",
    "            'last_letter': word[-1],\n",
    "            'length_of_name': len(word),\n",
    "            'contains_double': double_letter_check(word)}\n",
    "\n",
    "# Create fifth devtest and training set split\n",
    "random.shuffle(trainset_names)\n",
    "devtest_names_5 = trainset_names[:500]\n",
    "train_names_5 = trainset_names[500:]\n",
    "devtest_featuresets_5 = [(gender_features_iter_5(n), g) for (n,g) in devtest_names_5]\n",
    "train_featuresets_5 = [(gender_features_iter_5(n), g) for (n,g) in train_names_5]\n",
    "\n",
    "# Train the classifier on the training set and print the accuracy\n",
    "classifier_5 = nltk.MaxentClassifier.train(train_featuresets_5)\n",
    "print(nltk.classify.accuracy(classifier_5, devtest_featuresets_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.45998        0.755\n",
      "             3          -0.39830        0.803\n",
      "             4          -0.36733        0.814\n",
      "             5          -0.34934        0.817\n",
      "             6          -0.33777        0.817\n",
      "             7          -0.32977        0.819\n",
      "             8          -0.32396        0.820\n",
      "             9          -0.31955        0.821\n",
      "            10          -0.31611        0.821\n",
      "            11          -0.31333        0.820\n",
      "            12          -0.31105        0.821\n",
      "            13          -0.30912        0.821\n",
      "            14          -0.30748        0.822\n",
      "            15          -0.30605        0.822\n",
      "            16          -0.30480        0.822\n",
      "            17          -0.30369        0.822\n",
      "            18          -0.30269        0.821\n",
      "            19          -0.30179        0.821\n",
      "            20          -0.30097        0.821\n",
      "            21          -0.30023        0.822\n",
      "            22          -0.29954        0.821\n",
      "            23          -0.29891        0.821\n",
      "            24          -0.29833        0.821\n",
      "            25          -0.29779        0.821\n",
      "            26          -0.29728        0.821\n",
      "            27          -0.29681        0.821\n",
      "            28          -0.29637        0.822\n",
      "            29          -0.29596        0.822\n",
      "            30          -0.29557        0.822\n",
      "            31          -0.29521        0.821\n",
      "            32          -0.29486        0.821\n",
      "            33          -0.29453        0.822\n",
      "            34          -0.29423        0.821\n",
      "            35          -0.29393        0.821\n",
      "            36          -0.29366        0.821\n",
      "            37          -0.29339        0.821\n",
      "            38          -0.29314        0.821\n",
      "            39          -0.29290        0.821\n",
      "            40          -0.29267        0.821\n",
      "            41          -0.29245        0.821\n",
      "            42          -0.29225        0.822\n",
      "            43          -0.29205        0.821\n",
      "            44          -0.29185        0.821\n",
      "            45          -0.29167        0.821\n",
      "            46          -0.29149        0.821\n",
      "            47          -0.29133        0.821\n",
      "            48          -0.29116        0.821\n",
      "            49          -0.29101        0.821\n",
      "            50          -0.29086        0.821\n",
      "            51          -0.29071        0.821\n",
      "            52          -0.29057        0.821\n",
      "            53          -0.29044        0.821\n",
      "            54          -0.29031        0.821\n",
      "            55          -0.29018        0.821\n",
      "            56          -0.29006        0.822\n",
      "            57          -0.28994        0.821\n",
      "            58          -0.28983        0.821\n",
      "            59          -0.28972        0.821\n",
      "            60          -0.28961        0.821\n",
      "            61          -0.28951        0.821\n",
      "            62          -0.28941        0.821\n",
      "            63          -0.28931        0.821\n",
      "            64          -0.28922        0.821\n",
      "            65          -0.28913        0.821\n",
      "            66          -0.28904        0.821\n",
      "            67          -0.28895        0.821\n",
      "            68          -0.28887        0.821\n",
      "            69          -0.28879        0.821\n",
      "            70          -0.28871        0.821\n",
      "            71          -0.28863        0.821\n",
      "            72          -0.28855        0.821\n",
      "            73          -0.28848        0.821\n",
      "            74          -0.28841        0.821\n",
      "            75          -0.28834        0.821\n",
      "            76          -0.28827        0.821\n",
      "            77          -0.28821        0.821\n",
      "            78          -0.28814        0.821\n",
      "            79          -0.28808        0.821\n",
      "            80          -0.28802        0.821\n",
      "            81          -0.28796        0.821\n",
      "            82          -0.28790        0.821\n",
      "            83          -0.28784        0.821\n",
      "            84          -0.28779        0.821\n",
      "            85          -0.28773        0.821\n",
      "            86          -0.28768        0.821\n",
      "            87          -0.28763        0.821\n",
      "            88          -0.28758        0.821\n",
      "            89          -0.28753        0.821\n",
      "            90          -0.28748        0.821\n",
      "            91          -0.28743        0.821\n",
      "            92          -0.28738        0.821\n",
      "            93          -0.28734        0.821\n",
      "            94          -0.28729        0.821\n",
      "            95          -0.28725        0.821\n",
      "            96          -0.28720        0.821\n",
      "            97          -0.28716        0.821\n",
      "            98          -0.28712        0.821\n",
      "            99          -0.28708        0.821\n",
      "         Final          -0.28704        0.821\n",
      "0.774\n"
     ]
    }
   ],
   "source": [
    "# Define the features for the sixth classifier\n",
    "def gender_features_iter_6(word):\n",
    "    return {'first_letter': word[0].lower(),\n",
    "            'last_two_letters': word[-2:],\n",
    "            'length_of_name': len(word)}\n",
    "\n",
    "# Create fifth devtest and training set split\n",
    "random.shuffle(trainset_names)\n",
    "devtest_names_6 = trainset_names[:500]\n",
    "train_names_6 = trainset_names[500:]\n",
    "devtest_featuresets_6 = [(gender_features_iter_6(n), g) for (n,g) in devtest_names_6]\n",
    "train_featuresets_6 = [(gender_features_iter_6(n), g) for (n,g) in train_names_6]\n",
    "\n",
    "# Train the classifier on the training set and print the accuracy\n",
    "classifier_6 = nltk.MaxentClassifier.train(train_featuresets_6)\n",
    "print(nltk.classify.accuracy(classifier_6, devtest_featuresets_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that checks if a word contains a feminized ending\n",
    "def feminine_ending(word):\n",
    "    if word[-3:] == 'ina' or word[-2:] == 'ia' or word[-1] == 'a' or word[-3:] == 'lly':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.370\n",
      "             2          -0.47729        0.727\n",
      "             3          -0.43051        0.777\n",
      "             4          -0.40564        0.778\n",
      "             5          -0.39030        0.780\n",
      "             6          -0.38009        0.783\n",
      "             7          -0.37295        0.782\n",
      "             8          -0.36780        0.782\n",
      "             9          -0.36399        0.782\n",
      "            10          -0.36110        0.783\n",
      "            11          -0.35889        0.782\n",
      "            12          -0.35716        0.781\n",
      "            13          -0.35580        0.781\n",
      "            14          -0.35471        0.781\n",
      "            15          -0.35384        0.781\n",
      "            16          -0.35313        0.781\n",
      "            17          -0.35254        0.781\n",
      "            18          -0.35206        0.781\n",
      "            19          -0.35166        0.781\n",
      "            20          -0.35133        0.781\n",
      "            21          -0.35105        0.781\n",
      "            22          -0.35081        0.781\n",
      "            23          -0.35060        0.781\n",
      "            24          -0.35043        0.781\n",
      "            25          -0.35028        0.781\n",
      "            26          -0.35015        0.781\n",
      "            27          -0.35003        0.781\n",
      "            28          -0.34993        0.781\n",
      "            29          -0.34985        0.781\n",
      "            30          -0.34977        0.781\n",
      "            31          -0.34970        0.781\n",
      "            32          -0.34964        0.781\n",
      "            33          -0.34959        0.781\n",
      "            34          -0.34954        0.781\n",
      "            35          -0.34949        0.781\n",
      "            36          -0.34945        0.781\n",
      "            37          -0.34942        0.781\n",
      "            38          -0.34938        0.781\n",
      "            39          -0.34935        0.781\n",
      "            40          -0.34933        0.781\n",
      "            41          -0.34930        0.781\n",
      "            42          -0.34928        0.781\n",
      "            43          -0.34925        0.781\n",
      "            44          -0.34923        0.781\n",
      "            45          -0.34922        0.781\n",
      "            46          -0.34920        0.781\n",
      "            47          -0.34918        0.781\n",
      "            48          -0.34917        0.781\n",
      "            49          -0.34915        0.781\n",
      "            50          -0.34914        0.781\n",
      "            51          -0.34912        0.781\n",
      "            52          -0.34911        0.781\n",
      "            53          -0.34910        0.781\n",
      "            54          -0.34909        0.781\n",
      "            55          -0.34908        0.781\n",
      "            56          -0.34907        0.781\n",
      "            57          -0.34906        0.781\n",
      "            58          -0.34905        0.781\n",
      "            59          -0.34904        0.781\n",
      "            60          -0.34903        0.781\n",
      "            61          -0.34902        0.781\n",
      "            62          -0.34902        0.781\n",
      "            63          -0.34901        0.781\n",
      "            64          -0.34900        0.781\n",
      "            65          -0.34899        0.781\n",
      "            66          -0.34899        0.781\n",
      "            67          -0.34898        0.781\n",
      "            68          -0.34897        0.781\n",
      "            69          -0.34897        0.781\n",
      "            70          -0.34896        0.781\n",
      "            71          -0.34896        0.781\n",
      "            72          -0.34895        0.781\n",
      "            73          -0.34895        0.781\n",
      "            74          -0.34894        0.781\n",
      "            75          -0.34893        0.781\n",
      "            76          -0.34893        0.781\n",
      "            77          -0.34892        0.781\n",
      "            78          -0.34892        0.781\n",
      "            79          -0.34892        0.781\n",
      "            80          -0.34891        0.781\n",
      "            81          -0.34891        0.781\n",
      "            82          -0.34890        0.781\n",
      "            83          -0.34890        0.781\n",
      "            84          -0.34889        0.781\n",
      "            85          -0.34889        0.781\n",
      "            86          -0.34889        0.781\n",
      "            87          -0.34888        0.781\n",
      "            88          -0.34888        0.781\n",
      "            89          -0.34888        0.781\n",
      "            90          -0.34887        0.781\n",
      "            91          -0.34887        0.781\n",
      "            92          -0.34887        0.781\n",
      "            93          -0.34886        0.781\n",
      "            94          -0.34886        0.781\n",
      "            95          -0.34886        0.781\n",
      "            96          -0.34885        0.781\n",
      "            97          -0.34885        0.781\n",
      "            98          -0.34885        0.781\n",
      "            99          -0.34884        0.781\n",
      "         Final          -0.34884        0.781\n",
      "0.778\n"
     ]
    }
   ],
   "source": [
    "# Define the features for the seventh classifier\n",
    "def gender_features_iter_7(word):\n",
    "    return {'first_letter': word[0].lower(),\n",
    "            'last_letter': word[-1],\n",
    "            'length_of_name': len(word),\n",
    "            'feminine_ending': feminine_ending(word)}\n",
    "\n",
    "# Create seventh devtest and training set split\n",
    "random.shuffle(trainset_names)\n",
    "devtest_names_7 = trainset_names[:500]\n",
    "train_names_7 = trainset_names[500:]\n",
    "devtest_featuresets_7 = [(gender_features_iter_7(n), g) for (n,g) in devtest_names_7]\n",
    "train_featuresets_7 = [(gender_features_iter_7(n), g) for (n,g) in train_names_7]\n",
    "\n",
    "# Train the classifier on the training set and print the accuracy\n",
    "classifier_7 = nltk.MaxentClassifier.train(train_featuresets_7)\n",
    "print(nltk.classify.accuracy(classifier_7, devtest_featuresets_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature                                           female    male\n",
      "  ----------------------------------------------------------------\n",
      "  feminine_ending==True (1)                          0.256\n",
      "  last_letter=='y' (1)                               0.119\n",
      "  first_letter=='s' (1)                             -0.078\n",
      "  length_of_name==5 (1)                              0.004\n",
      "  feminine_ending==True (1)                                 -0.831\n",
      "  last_letter=='y' (1)                                      -0.149\n",
      "  first_letter=='s' (1)                                      0.127\n",
      "  length_of_name==5 (1)                                      0.036\n",
      "  -----------------------------------------------------------------\n",
      "  TOTAL:                                             0.300  -0.818\n",
      "  PROBS:                                             0.685   0.315\n"
     ]
    }
   ],
   "source": [
    "classifier_7.explain(gender_features_iter_7('Sally'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cunysps_data620",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
